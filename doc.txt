创建文本对话请求
Creates a model response for the given chat conversation.

POST
/
chat
/
completions

Try it
Authorizations
​
Authorization
string
header
required
Use the following format for authentication: Bearer <your api key>

Body
application/json
LLM模型
VLM模型
​
messages
object[]
required
A list of messages comprising the conversation so far.


Show child attributes

​
model
enum<string>
default:
deepseek-ai/DeepSeek-V3
required
对应的模型名称。为更好的提升服务质量，我们将不定期对本服务提供的模型做相关变更，包括但不限于模型上下线，模型服务能力调整，我们会在可行的情况下以公告、消息推送等适当的方式进行通知。

Available options: deepseek-ai/DeepSeek-R1, Pro/deepseek-ai/DeepSeek-R1, deepseek-ai/DeepSeek-V3, Pro/deepseek-ai/DeepSeek-V3, deepseek-ai/DeepSeek-R1-Distill-Llama-70B, deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, deepseek-ai/DeepSeek-R1-Distill-Llama-8B, deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, Pro/deepseek-ai/DeepSeek-R1-Distill-Llama-8B, Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, meta-llama/Llama-3.3-70B-Instruct, AIDC-AI/Marco-o1, deepseek-ai/DeepSeek-V2.5, Qwen/Qwen2.5-72B-Instruct-128K, Qwen/Qwen2.5-72B-Instruct, Qwen/Qwen2.5-32B-Instruct, Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct, Qwen/Qwen2.5-Coder-32B-Instruct, Qwen/Qwen2.5-Coder-7B-Instruct, Qwen/Qwen2-7B-Instruct, Qwen/Qwen2-1.5B-Instruct, Qwen/QwQ-32B-Preview, TeleAI/TeleChat2, 01-ai/Yi-1.5-34B-Chat-16K, 01-ai/Yi-1.5-9B-Chat-16K, 01-ai/Yi-1.5-6B-Chat, THUDM/glm-4-9b-chat, Vendor-A/Qwen/Qwen2.5-72B-Instruct, internlm/internlm2_5-7b-chat, internlm/internlm2_5-20b-chat, nvidia/Llama-3.1-Nemotron-70B-Instruct, meta-llama/Meta-Llama-3.1-405B-Instruct, meta-llama/Meta-Llama-3.1-70B-Instruct, meta-llama/Meta-Llama-3.1-8B-Instruct, google/gemma-2-27b-it, google/gemma-2-9b-it, Pro/Qwen/Qwen2.5-7B-Instruct, Pro/Qwen/Qwen2-7B-Instruct, Pro/Qwen/Qwen2-1.5B-Instruct, Pro/THUDM/chatglm3-6b, Pro/THUDM/glm-4-9b-chat, Pro/meta-llama/Meta-Llama-3.1-8B-Instruct, Pro/google/gemma-2-9b-it 
​
frequency_penalty
number
default:
0.5
​
max_tokens
integer
default:
512
The maximum number of tokens to generate.

Required range: 1 < x < 8192
​
n
integer
default:
1
Number of generations to return

​
response_format
object
An object specifying the format that the model must output.


Show child attributes

​
stop

Option 1 · string[] | null
Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.

​
stream
boolean
default:
false
If set, tokens are returned as Server-Sent Events as they are made available. Stream terminates with data: [DONE]

​
temperature
number
default:
0.7
Determines the degree of randomness in the response.

​
tools
object[]
A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.


Show child attributes

​
top_k
number
default:
50
​
top_p
number
default:
0.7
The top_p (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities.

Response
200 - application/json
​
choices
object[]

Show child attributes

​
created
integer
​
id
string
​
model
string
​
object
enum<string>
Available options: chat.completion 
​
tool_calls
object[]
The tool calls generated by the model, such as function calls.


Show child attributes

​
usage
object

Show child attributes

创建嵌入请求
由Mintlify提供支持

cURL

Python

JavaScript

Go

Java

const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"model":"deepseek-ai/DeepSeek-V3","messages":[{"role":"user","content":"中国大模型行业2025年将会迎来哪些机遇和挑战？"}],"stream":false,"max_tokens":512,"stop":["null"],"temperature":0.7,"top_p":0.7,"top_k":50,"frequency_penalty":0.5,"n":1,"response_format":{"type":"text"},"tools":[{"type":"function","function":{"description":"<string>","name":"<string>","parameters":{},"strict":false}}]}'
};

fetch('https://api.siliconflow.cn/v1/chat/completions', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));

200

400

401

404

429

503

504

{
  "id": "<string>",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "<string>",
        "reasoning_content": "<string>"
      },
      "finish_reason": "stop"
    }
  ],
  "tool_calls": [
    {
      "id": "<string>",
      "type": "function",
      "function": {
        "name": "<string>",
        "arguments": "<string>"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 123,
    "completion_tokens": 123,
    "total_tokens": 123
  },
  "created": 123,
  "model": "<string>",
  "object": "chat.completion"